{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN1G5DH80PVKMW2HCElKRLE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gshartnett/introAI/blob/main/homeworks/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Modern AI - HW 3\n",
        "**Student's Name**: YOUR NAME HERE  \n",
        "Instructor: Gavin Hartnett  \n",
        "PRGS, Winter Quarter 2022  \n",
        "\n",
        "This HW is worth 12.5% of your grade. Complete the assignment by making a local copy of this Colab Notebook and filling in the responses in your local copy. If you do not know how to typset math in LaTeX, feel free to email me a scanned piece of paper with your work instead. "
      ],
      "metadata": {
        "id": "T-k5eMJctnA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: CIFAR-100 Classification\n",
        "\n",
        "Train a CNN model on the CIFAR-100 classification task. Feel free to use the Week 4 Notebook as a guide (and recall that this notebook both shows how to download the CIFAR data and how to build and train a CNN on MNIST). Evaluate the test set accuracy after training it.\n",
        "\n",
        "One of the main goals for this exercise is to get you to work through how each layer of a CNN affects the shape/dimensionality of the data. The MNIST code will have to be modified to account for the fact that the CIFAR images are 32x32 and are also in color (c=3, as opposed to c=1). I suspect this will be the hardest part of this problem. I did not cover this in class, but you should be able to figure it out using the PyTorch docs pages and by making liberal use of inserting helpful debugging `print(x.shape)` statements throughout the `forward(self, x)` method. \n",
        "\n",
        "To help set you on the right path, this is how the input/output dimensions are related for some operations you will need to use:\n",
        "\n",
        "**[Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)**  \n",
        "- function call `Conv2d(Cin, Cout, KernelSize, Stride=1)`\n",
        "- Input shape: (N, Cin, Hin, Win)  \n",
        "- Output shape: (N, Cout, Hout, Wout)\n",
        "    - where Cout is a free parameter, and \n",
        "    - Hout = Hin - (KernelSize - Stride)  \n",
        "    - Wout = Win - (KernelSize - Stride)\n",
        "\n",
        "**[MaxPool2d](https://pytorch.org/docs/1.9.1/generated/torch.nn.MaxPool2d.html)**  \n",
        "`MaxPool2d(2)` cuts the image size by half. \n",
        "\n",
        "**Flatten**  \n",
        "At some point you will need to flatten the data to go from 4 dimensions to 2, i.e.\n",
        "- Input shape: (N, Cin, Hin, Win)\n",
        "- Output shape: (N, Cin * Hin * Win)"
      ],
      "metadata": {
        "id": "NfBOrewYAwPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here (feel free to use multiple cells)"
      ],
      "metadata": {
        "id": "VoWYiWjoZ6xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: Projected Gradient Descent (PGD)\n",
        "\n",
        "There is a modification of gradient descent which allows the method to be applied to *constrained* optimization problems. For example, suppose that we wish to minimize a function $f(\\boldsymbol{x})$ subject to the constraint that the point lie on the unit sphere, i.e., $x_1^2 + x_2^2 + x_3^2 = 1$. This can be accomplished through the use of a projection operator $P$, which takes a point in $\\mathbb{R}^3$ (the original, unconstrained space) and and projects it to the nearest point on the sphere $S^2$ (the constraint surface). The update rule for projected gradient descent is:\n",
        "\n",
        "$$ \\boldsymbol{x}_{k+1} = P\\left( \\boldsymbol{x}_k - \\alpha \\nabla f(\\boldsymbol{x}_k) \\right) \\,. $$\n",
        "\n",
        "In other words, we carry out a standard gradient descent step, which may take us off the constraint surface, and to correct this we then project down to the nearest point on the surface. This approach, PGD, is widely used in the construction of so-called *adversarial examples*, which you may have heard of. These will be covered in the next course in the AI/Data Science sequence."
      ],
      "metadata": {
        "id": "qKLJLdtSRxkG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)**  \n",
        "What is the projection operator here? In other words, given a point $\\boldsymbol{x} = (x_1, x_2, x_3) \\in \\mathbb{R}^3$, what is the map to the nearest point on the sphere? Code this up in PyTorch. \n",
        "\n",
        "Hint: if the answer isn't immediately obvious, try drawing a picture. $P$ will end up being a pretty simple function. "
      ],
      "metadata": {
        "id": "6aaXQ3_6gVj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "x_ykpYo6hHTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "For this exercise, we will need a function to minimize. By Googling around, I found a cute one [here](https://www.juliahomotopycontinuation.org/examples/optimization/):\n",
        "\n",
        "$$ f(\\boldsymbol{x}) = 3 x_1^3 x_2 + x_2^2 x_3^2 - 2 x_1 x_2 - 4 x_1 x_3^3 \\,. $$\n",
        "\n",
        "Implement this function, again in PyTorch. "
      ],
      "metadata": {
        "id": "gY8fhW5chJFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "aBV02CRARzNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**  \n",
        "Implement projected gradient descent for the function $f$, subject to the constaint that the point lie on the sphere. Initialize with a randomly chosen point on the sphere, and use PyTorch's autodiff to compute the gradient.\n",
        "\n",
        "Hints: \n",
        "- `x` will need to be a PyTorch tensor with `requires_grad=True`. \n",
        "- When performing the update step, we need to do so within a `torch.no_grad()` environment. This is because we are looping the result of the autodiff to then alter the inputs, which will then be used to once again compute a gradient, and so on. To help, I'll include a template for how your code should look:\n",
        "\n",
        "```\n",
        "def PGD_template():\n",
        "    init x\n",
        "    set requires_grad=True for x\n",
        "    loop over a number of iterations\n",
        "        compute the gradient\n",
        "        with torch.no_grad():\n",
        "            update x\n",
        "        set requires_grad=True for x again\n",
        "    return x, f(x)\n",
        "```"
      ],
      "metadata": {
        "id": "tKQezBldhpCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "iHi093aNW_1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D)**  \n",
        "Carry out multiple runs of your PGD code, each with a different random initialization. What is the global minimum?"
      ],
      "metadata": {
        "id": "UQQKh-_DieUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "LlHxB-_sVdn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part E)**  \n",
        "This problem is simple enough that we can solve it many other ways. For example, we can solve it using regular gradient descent without constraints by imposing directly that $\\boldsymbol{x}$ lie on the sphere, which can be accomplished via:\n",
        "\n",
        "$$ x_1 = \\sin\\theta \\cos\\phi \\,, \\quad x_2 = \\sin\\theta \\sin\\phi \\,, \\quad x_3 = \\cos\\theta \\,, \\quad \\text{where} \\quad \\theta \\in [0, \\pi], \\quad \\phi \\in [0, 2\\pi] \\,.$$\n",
        "\n",
        "Build a PyTorch implementation of the function $h(\\theta, \\phi) = f(x_1(\\theta, \\phi), x_2(\\theta, \\phi), x_3(\\theta))$ and optimize it using regular gradient descent (i.e., drop the $P$ operation). Hopefully you get the same answer for the minimal value of $f$. Also, I hope this helps to impress you with the power of autodiff - working out $\\nabla_x f$ doesn't seem too bad, but working out $\\nabla_{\\Omega} h$, where $\\Omega = (\\theta, \\phi)$, would be much more tedious. "
      ],
      "metadata": {
        "id": "H1vKIWBMDUrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "EwiOuzVlecL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_star = [np.sin(angles_star[0]) * np.cos(angles_star[1]),\n",
        "          np.sin(angles_star[0]) * np.sin(angles_star[1]),\n",
        "          np.cos(angles_star[0])]\n",
        "\n",
        "x_star "
      ],
      "metadata": {
        "id": "Uy_XPi9QELiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EKbko-ndGj_k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}