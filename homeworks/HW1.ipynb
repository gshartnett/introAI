{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPmCt1b7MzqLew1y6fixpQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gshartnett/introAI/blob/main/homeworks/HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Introduction to Modern AI - HW 1\n",
        "Gavin Hartnett  \n",
        "PRGS, Winter Quarter 2021\n",
        "\n",
        "This HW is worth 12.5% of your grade. Complete the assignment by making a local copy of this Colab Notebook and filling in the responses in your local copy. If you do not know how to typset math in LaTeX, feel free to email me a scanned piece of paper with your work instead. \n",
        "\n",
        "**Due: Friday, Jan 14 by 5pm (Pacific time)**"
      ],
      "metadata": {
        "id": "T-k5eMJctnA6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Problem 1: Linear Regression"
      ],
      "metadata": {
        "id": "B2zGMk55uIUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)** \n",
        "In class we discussed how to fit linear regression by minimizing the mean squared error (MSE). The equations that determine the fit parameters $(\\boldsymbol{w}_*, b_*)$ are\n",
        "\n",
        "$$ \\nabla_{\\boldsymbol{w}} \\text{MSE}(\\boldsymbol{w}, b) \\Big|_{(\\boldsymbol{w}^*, b^*)} = 0 \\,, \\qquad \\nabla_{b} \\text{MSE}(\\boldsymbol{w}, b) \\Big|_{(\\boldsymbol{w}^*, b^*)} = 0 \\,.$$\n",
        "\n",
        "If you've ever done a linear regression before, you probably used a software package that solved these equations (and did many other things, such as computing the $R^2$ coefficient). For this problem, we will derive the solution ourselves. To make our life easier, let's ignore the bias term, and set $b=0$. Work out the derivative of the MSE with respect to the weight vector $\\boldsymbol{w}$ and solve for the best-fit value $\\boldsymbol{w}_*$. \n",
        "\n",
        "Note: this question is meant to gauge your multi-variate calculus ability. If you are unduly struggling, please reach out to me or another student to get help. "
      ],
      "metadata": {
        "id": "ZYz1Wkwpfisg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "X3VaozRPhrPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider applying this to a real-world dataset. "
      ],
      "metadata": {
        "id": "3D6FbMHghzJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## imports\n",
        "\n",
        "## numpy and scikit-learn will be useful libraries for this exercise\n",
        "import numpy as np\n",
        "import sklearn.datasets\n",
        "\n",
        "## plotting library\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## some commands to make the plots look nicer\n",
        "plt.style.use('seaborn-white')"
      ],
      "metadata": {
        "id": "6KL2mOOJtykW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## increase the font size\n",
        "matplotlib.rcParams.update({'font.size': 16})"
      ],
      "metadata": {
        "id": "zYf2VOhS9gHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's load an example dataset from the scikit-learn (sklearn) library\n",
        "dataset = sklearn.datasets.load_diabetes()"
      ],
      "metadata": {
        "id": "HysEJNwt5iHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the dataset object is actually a Python dictionary\n",
        "dataset.keys()"
      ],
      "metadata": {
        "id": "JPoHw1C165hp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Inspect the dimensionality of the data\n",
        "dataset.data.shape"
      ],
      "metadata": {
        "id": "83J74vTY68rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The 10 features are\n",
        "dataset.feature_names"
      ],
      "metadata": {
        "id": "oD8Vd-gxEwFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## extract the (X,Y)'s\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "## define the dimensions of the data\n",
        "N = X.shape[0]\n",
        "p = X.shape[1]"
      ],
      "metadata": {
        "id": "a7GxgNPAyRpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier, we set the bias term $b=0$ in doing our derivation in order to make our lives easier. Actually, there's a little trick we can do to restore this term in our solution without doing any additional work. \n",
        "\n",
        "If $\\boldsymbol{x}$ is a $p$-dim vector, let's consider the enlarged $(p+1)$-dim vector given by\n",
        "\n",
        "$$ \\boldsymbol{x}' = (\\boldsymbol{x}, 1) \\,. $$\n",
        "\n",
        "Let's also enlarge the vector of weights in the same way,\n",
        "\n",
        "$$ \\boldsymbol{w}' = (\\boldsymbol{w}, b) \\,. $$\n",
        "\n",
        "Now notice that the linear model (in terms of the primed variables) with no bias term naturally incorporates a bias term when expressed in terms of the original vectors:\n",
        "\n",
        "$$ \\boldsymbol{w}'^T \\boldsymbol{x}' = \\boldsymbol{w}^T \\boldsymbol{x} + b \\,,$$\n",
        "\n",
        "In other words, we were able to set $b=0$ above with really no loss in generality, and we can easily accomodate the bias term by simply adding 1 to each $\\boldsymbol{x}$ value in our dataset. If we want to do this for every single data point at once, we can consider the $N\\times p$ matrix $X$ and just add a column vector of 1's to it. "
      ],
      "metadata": {
        "id": "Z2PTozRHWGWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## add a column vector of 1's to the X matrix\n",
        "## this lets us incorporate a bias term easily\n",
        "X_enlarged = np.concatenate((X, np.ones(N)[:,None]), axis=1)\n",
        "X_enlarged.shape"
      ],
      "metadata": {
        "id": "8-cxBYeAH8j_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## shuffle the data\n",
        "## this is just good practice\n",
        "permutation = np.arange(N)\n",
        "np.random.shuffle(permutation) #this is an in-place operation\n",
        "\n",
        "X = X[permutation,:]\n",
        "X_enlarged = X_enlarged[permutation,:]\n",
        "y = y[permutation]"
      ],
      "metadata": {
        "id": "apgc93HG84DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**\n",
        "\n",
        "Implement the above solution you derived for the OLS linear regression fit to the data. You will find the following numpy commands helpful:\n",
        "\n",
        "np.transpose(X) - transpose the matrix X  \n",
        "np.linalg.inverse(X) - invert the matrix X  \n",
        "np.dot(A, B) - find the dot product between two matrices, vectors, or matrix-vector pair. \n",
        "\n"
      ],
      "metadata": {
        "id": "fGGj1BKGFjfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "m3TGpybOyyef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**\n",
        "\n",
        "Now let's compare this to the answer we get when we use a built-in linear regression solver. Use scikit-learn to fit a regression model to the data and verify that the fitted parameters it returns agree with our wstar solution above. \n",
        "\n",
        "Hints\n",
        "- Feel free to use the [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) to see how to use their linear regression solver. This skill, being able to g to a docs page and teach yourself how to use a particular software package, is a key part of doing ML/data science. \n",
        "- We need to be careful to use the original (not the enlarged) X matrix."
      ],
      "metadata": {
        "id": "-OJ4PIQHLaCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "owZ3mHfsHQeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D)**\n",
        "\n",
        "Finally, let's try to gain a sense for the MSE loss function. What does it *look* like? Well, it's a bit hard to visualize because it's a function of 11 parameters (10 weights and a bias). One solution is to just look at a 1d slice of this 11d space - of course this can't give us the whole picture, but it's better than nothing. \n",
        "\n",
        "There are a lot of ways to pick the 1d slice. A natural one is to look at the loss function along the ray picked out by our optimal solution, i.e. \n",
        "\n",
        "$$ \\boldsymbol{w}(t) = \\boldsymbol{w}_* t \\,, \\quad t \\in \\mathbb{R} \\,.  $$\n",
        "\n",
        "This is a family of vectors (directions). For $t=0$ it's the origin, and for $t=1$ it's the best-fit vector. Define a Python function for the MSE as a function of $t$ and plot it. For what value of $t$ is the MSE the smallest? Does this make sense?"
      ],
      "metadata": {
        "id": "81LGjqO8PaQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "hmm20dMHPzu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without giving too much away, your plot should look like a \"U\" shape. Of course, we can't conclude much about the full 11-d shape of the loss just from this 1d slice, but it turns out that in this case such an unjustified extrapolation actually gives the right answer. The full loss function looks like a big bowl in 11d space. This is because the loss function is a quadratic polynomial - take the linear model $f$ and square it to build the MSE."
      ],
      "metadata": {
        "id": "XUW-WikpIcE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Debrief**\n",
        "\n",
        "Congratulations! You just implemented from scratch your first basic machine learning model! Normally we'd be interested in evaluating the performance on the model on unseen data, but we've beaten this linear regression horse enough for today."
      ],
      "metadata": {
        "id": "nT9DlMGbL5bh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Problem 2: (Binary) Logistic Regression\n",
        "\n",
        "Now we will explore some basic properties of logistic regression, again using one of the included sample datasets in the scikit-learn library.\n",
        "\n",
        "As much as I'd love to ask you to derive the optimal, best-fit solution as I did for the linear regression problem, I can't, because a closed-form solution does not exist. Even though logistic regression is still a linear model, the loss function is now more complicated than before, and involves non-linearities (such as $\\ln$, $\\exp$) which complicate the expression enough to preclude a simple closed form solution.  "
      ],
      "metadata": {
        "id": "HPBYSUHCd0fZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## let's load an example dataset from the scikit-learn (sklearn) library\n",
        "dataset = sklearn.datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "1GlZjb-Md1vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the dataset object is actually a Python dictionary\n",
        "dataset.keys()"
      ],
      "metadata": {
        "id": "-bY49NlAKcZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Inspect the dimensionality of the data\n",
        "dataset.data.shape"
      ],
      "metadata": {
        "id": "9s9e3_KXKe79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The 30 features are\n",
        "dataset.feature_names"
      ],
      "metadata": {
        "id": "XhEK51JaKiRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## extract the (X,Y)'s\n",
        "X = dataset.data\n",
        "y = dataset.target\n",
        "\n",
        "## define the dimensions of the data\n",
        "N = X.shape[0]\n",
        "p = X.shape[1]"
      ],
      "metadata": {
        "id": "txBnNLEyKm1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## shuffle the data\n",
        "## this is just good practice\n",
        "permutation = np.arange(N)\n",
        "np.random.shuffle(permutation) #this is an in-place operation\n",
        "\n",
        "X = X[permutation,:]\n",
        "y = y[permutation]"
      ],
      "metadata": {
        "id": "wsVklLZmL0RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## for the logistic regression to converge we need to scale the data\n",
        "## see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "## for more details if you're curious\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X  = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "FIt1vHntMeoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the y-values are binary\n",
        "y"
      ],
      "metadata": {
        "id": "11Ufr823KqPs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## split the data into a test and train set\n",
        "Ntrain = int(0.8 * N) # let's do an 80/20 split\n",
        "Ntest = N - Ntrain\n",
        "\n",
        "X_train = X[:Ntrain]\n",
        "y_train = y[:Ntrain]\n",
        "\n",
        "X_test = X[Ntrain:]\n",
        "y_test = y[Ntrain:]"
      ],
      "metadata": {
        "id": "ZwCdzgbPLQQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)**\n",
        "\n",
        "Use scikit-learn to fit a logistic regression model to the *training* data. \n",
        "\n",
        "Hints\n",
        "- Feel free to use the [scikit-learn docs](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) to see how to use their logistic regression solver. "
      ],
      "metadata": {
        "id": "2e9YUWa5K71k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "bQ0jMCGCKwis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**\n",
        "\n",
        "Having fit the model to the training data, let's look at the predictions for the as-yet unseen testing data. This can be accomplished by running ```model.predict(X_test)```, if you called your scikit-learn model ```model```. \n",
        "\n",
        "If you recall, in class I actually never explained how to get the predictions from this model. I explained how we get the estimate for the probabilities $p(y=1|\\boldsymbol{x})$. These can be obtained using ```model.predict_proba(X_test)```. \n",
        "\n",
        "Question: Explain how the predictions are related to the probability estimates. Verify that your answer is correct by checking that it correctly reproduces the scikit-learn predictions when applied to the scikit-learn probabilities. "
      ],
      "metadata": {
        "id": "qKyOUWeDNLYU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "34FPfhlHO1ht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "jQ--iLs_LlH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's compute the test set accuracy\n",
        "np.mean(y_test == y_pred)"
      ],
      "metadata": {
        "id": "9IPf7lafQCUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**\n",
        "\n",
        "Finally, let's investigate the ROC curve for our model. Use the [built-in scikit-learn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) to plot this. What can you learn from the performance of the model by looking at this plot?"
      ],
      "metadata": {
        "id": "jFR5cwWvTAtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "_aLVGMsgQGEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D)**  \n",
        "In class I explained that the loss function for logisic regression is the Binary Cross Entropy (BCE) loss. You may be wondering where this particular loss function comes from. \n",
        "\n",
        "Show that it corresponds to the Negative Log Likelihood (NLL) (up to a constant), so that minimizing the BCE loss corresponds to maximizing the likelihood. To help you get started, here are expressions for BCE and the likelihood L:\n",
        "\n",
        "$$\\text{BCE} = - \\frac{1}{N} \\sum_{i=1}^N \\Big( y_i \\ln p(y=1|x_i) + (1-y_i) \\ln \\left(1 - p(y=1|x_i) \\right) \\Big) $$\n",
        "\n",
        "$$ L = \\prod_{i=1}^N p(y_i |x_i) $$"
      ],
      "metadata": {
        "id": "_kza11e8Acbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "i89V10UXCU5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Rhk2gs_yBpo2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}