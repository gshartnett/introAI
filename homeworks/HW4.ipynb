{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkPJ8rHoHt45gFlDZIZQHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gshartnett/introAI/blob/main/homeworks/HW4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Modern AI - HW 4\n",
        "**Student's Name**: YOUR NAME HERE  \n",
        "Instructor: Gavin Hartnett  \n",
        "PRGS, Winter Quarter 2022  \n",
        "\n",
        "This HW is worth 12.5% of your grade. Complete the assignment by making a local copy of this Colab Notebook and filling in the responses in your local copy. If you do not know how to typset math in LaTeX, feel free to email me a scanned piece of paper with your work instead. "
      ],
      "metadata": {
        "id": "T-k5eMJctnA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## imports\n",
        "\n",
        "## numpy\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng(123)\n",
        "\n",
        "## other useful data analysis libraries\n",
        "import pandas as pd\n",
        "from sklearn import neighbors\n",
        "\n",
        "## plotting\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## torch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "h6SXn09PKNDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## some commands to make the plots look nicer\n",
        "plt.style.use('seaborn-white')\n",
        "matplotlib.rcParams.update({'font.size': 16})"
      ],
      "metadata": {
        "id": "KImOGhPdKQrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## set the device variable\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "Et42DKnPKSVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "pBsuWBRZ9tFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1: PCA\n",
        "\n",
        "**Part A)**  \n",
        "Following my example in the Week 4 Notebook, download the *training part* of the [Fashion MNIST](https://pytorch.org/vision/stable/datasets.html#fashion-mnist) dataset. (This time, without any transformations applied). \n",
        "- What are the dimensions of the images?\n",
        "- Make a 10x10 plot of 100 random images to get a sense for what the dataset looks like. "
      ],
      "metadata": {
        "id": "g3RdScoPHBDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "7aG7G3zpLjC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "Using any approach you find convenient, organize the data into one large numpy array $X$. Flatten the pixels so that $X$ is a matrix with dimensions $N \\times p$, where $N$ is the number of images in the training set and $p=n^2$ is the square of the pixel size. Also collect all the labels in a $N$-dimensional vector $y$. "
      ],
      "metadata": {
        "id": "0grXfF7qQOjp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "B7oGZPswRM6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**  \n",
        "Apply the [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) implementation of PCA to the data array $X$ from the previous part. Project the data down to the first 2 principal components. The result should be a $N \\times 2$ dimensional array `X_reduced`. \n"
      ],
      "metadata": {
        "id": "zsefHDFNSDFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "Y_RRZgxuRM9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "60,000 data points is going to be a bit unwieldy for what we're going to do next, so let's just grab the first 5000 datapoints."
      ],
      "metadata": {
        "id": "eOcxfnXBWvAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_reduced = X_reduced[0:5000, :]\n",
        "y = y[0:5000]"
      ],
      "metadata": {
        "id": "SBnRqUmiW3Hf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2: K-Means Clustering\n",
        "\n",
        "In this problem we will implement the K-means clustering algorithm and apply it to the PCA-reduced Fashion MNIST dataset. Recall that K-means is an unsupervised algorithm designed to find the best clustering assignments based on minimizing the squared Euclidean distance between data points. By setting $K=10$, we will investigate to what extent these clusters correspond to the labels within the Fashion MNIST dataset. \n",
        "\n",
        "**Part A)**\n",
        "Implement *from scratch* K-means clustering with $K=10$. The pseudocode version of the algorithm is:\n",
        "- Initialize the cluster assignments $c(i)$ for $i=1,...,N$\n",
        "- Initialize the means or centroids, $\\boldsymbol{\\mu}_k$ for $k=1,...,K$. \n",
        "- Until convergence, do:\n",
        "    - for each $i=1,...,N$, update  \n",
        "$$c(i) = \\text{argmin}_k || \\boldsymbol{x}_i - \\boldsymbol{\\mu}_k||^2$$\n",
        "    - for each $k=1,...,K$, update  \n",
        "$$\\boldsymbol{\\mu}_k = \\frac{1}{N_k} \\sum_{i: c(i)=k} \\boldsymbol{x}_i$$\n",
        "\n",
        "This part will be worth the same number of points as 3 ordinary parts (6 points total)."
      ],
      "metadata": {
        "id": "TPC6fljHTvnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here\n",
        "## Note: the final output should be a list of label assignments label_preds for each i=1,...,N"
      ],
      "metadata": {
        "id": "Q_-DJna-GHIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's plot the result. The below code should run if `X_reduced` and `label_preds` are in the proper format. "
      ],
      "metadata": {
        "id": "XQq1clfBWN5p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
        "h = 0.02  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "\n",
        "# Plot the decision boundary. For that, we will assign a color to each\n",
        "x_min, x_max = X_reduced[:, 0].min() - 1, X_reduced[:, 0].max() + 1\n",
        "y_min, y_max = X_reduced[:, 1].min() - 1, X_reduced[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "# Obtain labels for each point in mesh. Use last trained model.\n",
        "Z = model.predict_labels(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the K-means decision boundaries\n",
        "fig = plt.figure(figsize=(10,10))\n",
        "plt.imshow(\n",
        "    Z,\n",
        "    interpolation=\"nearest\",\n",
        "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
        "    cmap=plt.cm.Paired,\n",
        "    aspect=\"auto\",\n",
        "    origin=\"lower\",\n",
        ")\n",
        "\n",
        "## Plot the raw data\n",
        "plt.plot(X_reduced[:, 0], X_reduced[:, 1], \"k.\", markersize=2)\n",
        "\n",
        "## Plot the centroids as a white X\n",
        "centroids = model.centroids\n",
        "plt.scatter(\n",
        "    centroids[:, 0],\n",
        "    centroids[:, 1],\n",
        "    marker=\"x\",\n",
        "    s=200,\n",
        "    color=\"w\",\n",
        "    zorder=10,\n",
        "    linewidth=1\n",
        ")\n",
        "\n",
        "## Make the plot pretty\n",
        "plt.title(\n",
        "    \"K-means clustering on PCA-reduced Fashion MNIST\\n\"\n",
        "    \"Centroids are marked with white cross\"\n",
        ")\n",
        "plt.xlim(x_min, x_max)\n",
        "plt.ylim(y_min, y_max)\n",
        "#plt.xticks(())\n",
        "#plt.yticks(())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4qbVJybJHtzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is perhaps an unusual application of K-means as in this case we already know a good labeling/clustering scheme, namely the labels associated with the original dataset. The code block below maps our 10 K-means labels to the true 10 labels and then checks our accuracy. "
      ],
      "metadata": {
        "id": "WrPzNfJnWjzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_matrix = np.zeros((10, 10))\n",
        "for i in range(X_reduced.shape[0]):\n",
        "    label_matrix[y[i], label_preds[i]] += 1\n",
        "\n",
        "argmax = np.argmax(label_matrix, axis=1)\n",
        "my_dic = {i:argmax[i] for i in range(len(argmax))}\n",
        "\n",
        "y_mapped = np.vectorize(my_dic.get)(y)\n",
        "print('accuracy of K-means: %.2f' %np.mean(y_mapped == label_preds))"
      ],
      "metadata": {
        "id": "Kmmykxt4GHK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "For fun (and so you know the easier way to do this in the future), use [scikit-learn's implementation of K-means](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html), and then make the same plot as above."
      ],
      "metadata": {
        "id": "k39Uyc56FYN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "ez1K0VHoM8PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3: Policy2Vec\n",
        "\n",
        "In this problem you will gain experience applying [Gensim's implementation of *doc2vec*](https://radimrehurek.com/gensim/models/doc2vec.html) to policy-relevant documents. This problem is inspired by [some work I did a few years ago](https://github.com/RANDCorporation/policy2vec).\n",
        "\n",
        "**Part A)**  \n",
        "For this problem we will look at Executive Orders, which can be obtained [here](https://www.federalregister.gov/presidential-documents/executive-orders). This website will allow you to download a .csv file containing the urls of individual Executive Order pdf files. I went through the trouble of downloading all these and converting them to raw text form. You can find this data in the Teams folder. Download the data to your local machine, and upload it using the code below. \n",
        "\n",
        "Note: I'm being easy on you - you don't need to write any code, just follow the instructions and upload the data correctly."
      ],
      "metadata": {
        "id": "bCleJNJ_SXKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "YBVERmpgYKFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## the files are uploaded into a dict, put them into a list of filenames and texts\n",
        "filenames = list(uploaded.keys())\n",
        "texts = list(uploaded.values())"
      ],
      "metadata": {
        "id": "gE2lzSvPd8-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0][0:10]"
      ],
      "metadata": {
        "id": "yz79a75FfJNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text was uploaded as a bytes object (hence the b'xxxx' format). Let's convert this to a string format."
      ],
      "metadata": {
        "id": "886NH-mtfL7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(texts)):\n",
        "    texts[i] = texts[i].decode('utf-8')"
      ],
      "metadata": {
        "id": "GCwIYVPefWku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0][0:10]"
      ],
      "metadata": {
        "id": "UQvJW3Guff_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's inspect the text\n",
        "print(texts[0][2000:3000])"
      ],
      "metadata": {
        "id": "sAf8QJrzeBGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "Preprocess the text. I'm going to leave this rather open-ended to encourage you to play with it, since as I mentioned in class, preprocessing often ends up being the task you spend the most time on. Also, differences in how the data is preprocessed can often be more influential than the choice of algorithm. At the bare minimum, the text needs to be tokenized, i.e., \n",
        "\n",
        "`data = [data[0], data[1], data[2], ..., data[n]]`\n",
        "\n",
        "where \n",
        "\n",
        "`data[i] = [token 0 of document i, token 1 of document i, ... ]`\n",
        "\n",
        "and where n = 1, ..., number of executive order documents. As an example, if the texts were \"see spot run\", \"he is fast\", then we want something like\n",
        "\n",
        "`data = [['see', 'spot', 'run'], ['he', 'is', 'fast']]`.\n",
        "\n",
        "Additional things to explore are:\n",
        "- removing stop words\n",
        "- lemmatizing\n",
        "- removing whitespace, special characters\n",
        "- changing to lower-case\n",
        "\n"
      ],
      "metadata": {
        "id": "IsIUa4o5XHCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "rnfejS3OkKFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**  \n",
        "Train a Doc2Vec model using Gensim. This can be done with one line of code, see this [docs page](https://radimrehurek.com/gensim/models/doc2vec.html\n",
        ") for help."
      ],
      "metadata": {
        "id": "zJ3M2KHykJp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## first we need to format the data in the way Gensim expects\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data)]\n",
        "\n",
        "## your code here"
      ],
      "metadata": {
        "id": "384z1Jjpi2VM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D)**  \n",
        "Define a function that implements the cosine similarity $S_C(\\boldsymbol{v}_1, \\boldsymbol{v}_2)$, for two vectors $\\boldsymbol{v}_1, \\boldsymbol{v}_2$."
      ],
      "metadata": {
        "id": "ROuc4jofpvWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "Z3SP7y_5p4vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part E)**  \n",
        "Use the `model.infer_vector()` method to find the EO document whose vector is most similar (in terms of cosine similarity) to:\n",
        "\n",
        "\"The US is considering sanctioning Russian oligarchs in response to the invasion of Crimea.\" \n",
        "\n",
        "This will require you to compute $S_C(v(d), v_*)$ for all $d \\in D$, where $v_*$ is the vector for the above prompt, and $v(d)$ is the vector for document $d$. Then you'll have to find the $d$ for which this similarity is largest.  "
      ],
      "metadata": {
        "id": "lS8AwbpooCUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The US is considering sanctioning Russian oligarchs connected with Vladimir Putin in response to the invasion of Crimea in Ukraine.'"
      ],
      "metadata": {
        "id": "EfahfI_NpWUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "KfOKs28yoB5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[imax])"
      ],
      "metadata": {
        "id": "p41zhJvaocaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yt4_nhvaqT7R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}