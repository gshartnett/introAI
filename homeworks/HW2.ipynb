{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO5axGZEqiskUCykkQGnW3h",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gshartnett/introAI/blob/main/homeworks/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Introduction to Modern AI - HW 2\n",
        "Gavin Hartnett  \n",
        "PRGS, Winter Quarter 2022\n",
        "\n",
        "This HW is worth 12.5% of your grade. Complete the assignment by making a local copy of this Colab Notebook and filling in the responses in your local copy. If you do not know how to typset math in LaTeX, feel free to email me a scanned piece of paper with your work instead. "
      ],
      "metadata": {
        "id": "T-k5eMJctnA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##imports\n",
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "rng = default_rng(123)\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "## some commands to make the plots look nicer\n",
        "plt.style.use('seaborn-white')"
      ],
      "metadata": {
        "id": "MnwLFrKCe-lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## increase the font size\n",
        "matplotlib.rcParams.update({'font.size': 16})"
      ],
      "metadata": {
        "id": "fiYMafBrhx4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Problem 1: The Perceptron Learning Algorithm\n",
        "In class we presented the Perceptron learning algorithm. Let's try to understand where that came from. Recall that the Perceptron model is:\n",
        "\n",
        "$$ \n",
        "f(x; w) = \\text{sign}( w^T x ) = \n",
        "\\begin{cases}\n",
        "-1 \\,, & w^T x < 0 \\\\\n",
        "+1 \\,, & w^T x > 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "(Also recall that here we are using the plus/minus convention for binary classification, i.e., $y \\in \\{-1,1\\}$). A natural loss function for binary classification is the 0-1 loss:\n",
        "\n",
        "$$ \n",
        "L_{0-1}(y, \\hat{y}) = \n",
        "\\begin{cases}\n",
        "0 & y = \\hat{y} \\\\\n",
        "1 & y \\neq \\hat{y}\n",
        "\\end{cases} \\,.\n",
        "$$\n",
        "\n",
        "This is simple enough - you get penalized 1 point for every mistake. This loss function has the nice property that its average corresponds to the error rate - i.e. the metric we often care most about. Of course, an obvious draw back of this loss function is that it's not differentiable (it's not even continuous, and the inputs are themselves discrete variables). So the loss function isn't as informative as we might like - it only tells us if we are right or wrong. \n",
        "\n",
        "Let's consider the following loss function:\n",
        "\n",
        "$$ L(y, \\hat{y}) = \\max\\left(0, - y \\, \\hat{y} \\right) \\,. $$\n",
        "\n",
        "This is the same as the 0-1 loss, just written in a more complicated way. Let's verify this:\n",
        "- If $y = \\hat{y}$ (so that $y \\hat{y} = 1$), then the loss is 0 \n",
        "- If $y = - \\hat{y}$ (so that $y \\hat{y} = -1$), the loss is 1. \n",
        "\n",
        "Even though we've done nothing but rewrite the old loss function, this new form suggests an improved, or \"softened\" loss function. Let's call the thing that goes into the sign function the score, or pre-activation: $\\hat{z} = w^T x$, so that $\\hat{y} = \\text{sign}(\\hat{z})$. If we replace $\\hat{y}$ with $\\hat{z}$ in the above loss function, \n",
        "\n",
        "$$ L(y, \\hat{z}) = \\max\\left(0, - y \\, \\hat{z} \\right) \\,, $$\n",
        "\n",
        "we now have a loss function that is differentiable. "
      ],
      "metadata": {
        "id": "ySa5nBQz0Of5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)**  \n",
        "\n",
        "Plot the loss $ L(y, \\hat{z}) = \\max\\left(0, - y \\, \\hat{z} \\right)$ as a function of $\\hat{z}$ for both $y=1$ and $y=-1$. "
      ],
      "metadata": {
        "id": "NxNui9F4gazR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "sBl_ssJBbb0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot tells us that the loss penalty (for incorrect predictions) increases linearly as the prediction gets worse. If the prediction is correct, there is no loss penalty."
      ],
      "metadata": {
        "id": "WGLG3bu4s7Bl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "Derive the gradient of the loss $L(y, \\hat{z})$ with respect to the weights $w$."
      ],
      "metadata": {
        "id": "asC6aEwKhkKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "ZPxssrB1h5CW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**  \n",
        "Having derived the gradient of the loss function, what is the SGD update rule for the weights? "
      ],
      "metadata": {
        "id": "AMskrrR0moim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "sa7pOVxkmyhs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's implement this model \"from scratch\" (don't worry, I'll help guide you!). "
      ],
      "metadata": {
        "id": "uXTjd-RaM7NZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## import a fake dataset\n",
        "from sklearn.datasets import make_blobs\n",
        "N = 200\n",
        "p = 2\n",
        "X, y = make_blobs(n_samples=N, centers=p, n_features=2, cluster_std=0.3, random_state=0)\n",
        "\n",
        "## convert the y-values to {-1, +1}\n",
        "y = 2*y - 1\n",
        "\n",
        "## enlarge the X-values to account for the bias term as we did in HW1.\n",
        "X_enlarged = np.concatenate((X, np.ones(N)[:,None]), axis=1) "
      ],
      "metadata": {
        "id": "cvIN1ky904F5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## plot the data\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "for i in [-1, 1]:\n",
        "    plt.scatter(X[y==i,0], X[y==i,1], label='label: %i' %i)\n",
        "plt.xlabel(r'$x$')\n",
        "plt.ylabel(r'$y$')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ddJ9nwAZm0Nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D)**  \n",
        "\n",
        "Below is a simple class for the Perceptron model. If you don't know what classes are in Python, this would be a good time to do some Googling. Classes come equipped with their own functions which are called methods. Complete the two methods ``predict`` and ``update_weights`` in the class below to implement the Perceptron prediction rule and update rule. \n",
        "- prediction rule: $\\hat{y} = \\text{sign}(w^T x)$\n",
        "- update rule: $\\Delta w = - \\alpha \\nabla_w L(y, w^T x)$\n",
        "\n",
        "Hint: when you call the weights within the method you need to use `self.w` rather than simply `w`."
      ],
      "metadata": {
        "id": "bpgOR5ChnLpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Perceptron():\n",
        "    '''This is a class for the Perceptron model.'''\n",
        "    def __init__(self, p, lr=0.05):\n",
        "        self.w = default_rng(4).standard_normal(size=p)\n",
        "        self.lr = lr #the learning rate\n",
        "\n",
        "    def predict(self, X):\n",
        "        ## your answer here\n",
        "\n",
        "    def update_weights(self, X, y):\n",
        "        ## your answer here"
      ],
      "metadata": {
        "id": "g6nk3dAtm0Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's train the model and visualize the result. The following code should successfully train the Perceptron if you've done the above exercises correctly. "
      ],
      "metadata": {
        "id": "m1m3B-unnQYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## init the model\n",
        "model = Perceptron(p+1)\n",
        "model.w"
      ],
      "metadata": {
        "id": "Om3VEnblnTSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 100\n",
        "\n",
        "## get the range of the data\n",
        "xmin = np.min(X[:,0])\n",
        "xmax = np.max(X[:,0])\n",
        "ymin = np.min(X[:,1])\n",
        "ymax = np.max(X[:,1])\n",
        "\n",
        "## build an array of x-values for plotting\n",
        "xlist = np.linspace(-5, 5, 1000)\n",
        "\n",
        "## halt the updating if perfect accuracy is achieved\n",
        "i = 0\n",
        "while np.mean(model.predict(X_enlarged) == y) < 1 and i < max_iters:\n",
        "\n",
        "    ## start a new plot for each iteration\n",
        "    fig, ax = plt.subplots(figsize=(9,6))\n",
        "\n",
        "    ## plot the raw data\n",
        "    for k in [-1, 1]:\n",
        "        ax.scatter(X[y==k,0], X[y==k,1])\n",
        "    ## color the current point in black\n",
        "    ax.scatter(X[i,0], X[i,1], c='k')\n",
        "\n",
        "    ## plot the old decision boundary\n",
        "    ylist_old = -model.w[0]*xlist/model.w[1] - model.w[2]/model.w[1]\n",
        "    ax.plot(xlist, ylist_old, color='k', alpha=1, linestyle='--', label='old')\n",
        "    ax.fill_between(xlist,  ylist_old, y2=-1000, color='orange', alpha=0.2)\n",
        "    ax.fill_between(xlist,  ylist_old, y2=1000, color='blue', alpha=0.2)    \n",
        "\n",
        "    ## update the model\n",
        "    yhat_i_old = model.predict(X_enlarged[i])\n",
        "    model.update_weights(X_enlarged[i], y[i])\n",
        "    yhat_i_new = model.predict(X_enlarged[i])\n",
        "\n",
        "    ## if prediction changes, plot the new decision boundary\n",
        "    if yhat_i_old != yhat_i_new:\n",
        "        ylist_new = -model.w[0]*xlist/model.w[1] - model.w[2]/model.w[1]\n",
        "        ax.plot(xlist, ylist_new, color='k', alpha=1, label='new')\n",
        "    \n",
        "    ax.set_xlabel(r'$x$')\n",
        "    ax.set_ylabel(r'$y$')\n",
        "    ax.set_title('iteration t = %i' %i)\n",
        "    ax.set_xlim([xmin - 0.5, xmax + 0.5])\n",
        "    ax.set_ylim([ymin - 0.5, ymax + 0.5])\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "9sPwXGfknW05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debrief: \n",
        "\n",
        "In addition to introducing some new concepts and testing your ability to implement some ML ideas in Python, some take-aways from this problem are that:\n",
        "- The Perceptron update rule/learning algorithm is just stochastic gradient descent\n",
        "- The loss function is one we hadn't encountered before, but it seems sensible as a surrogate loss for the 0-1 loss \n",
        "- The Perceptron is a close cousin to logistic regression, both are linear classifiers. Some key differences are the choice of loss function, and the activation (sigmoid vs sign). "
      ],
      "metadata": {
        "id": "PCmKqEYAnZ2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Problem 2: Bias-Variance Trade-Off\n",
        "\n",
        "In lecture we discussed the bias-variance trade-off. This is the idea that the error of a *any* model can be decomposed into 3 distinct components:\n",
        "- An irreducible loss, capturing the fact that even if we had a perfect model there will be error due to the noise\n",
        "- A bias loss, capturing the idea that our model might be biased towards the wrong class of functions (incorrectly modeling the signal). A good example of this would be using a linear model for a non-linear problem. \n",
        "- A variance loss, capturing the idea that even an unbiased model will move around its mean. \n",
        "\n",
        "In this problem you will derive the bias-variance decomposition for the MSE loss function.\n",
        "\n",
        "**Set-Up**  \n",
        "Here are some useful definitions and conventions you will need:\n",
        "- $\\mathbb{E}[X]$ will denote the expectation value (i.e., mean, or average) of a random variable $X$\n",
        "- When multiple random variables are involved, we sometimes use a subscript to indicate which variable the expectation refers to. For example, if $Y = f(X) + \\epsilon$, and both $X$ and $\\epsilon$ are random, then $\\mathbb{E}_{\\epsilon}[Y]$ indicates that we are averaging over the noise variable $\\epsilon$ and not the data variable $X$. \n",
        "- $\\text{Var}[X] = \\mathbb{E}\\left[ \\left( X - \\mathbb{E}[X] \\right)^2 \\right]$ is the variance of $X$\n",
        "- If we have an estimate $\\hat{x}$ of a random variable $X$, then the bias of the estimate is defined as $\\text{Bias}[\\hat{x}] = (\\hat{x} - \\mathbb{E}[X])$\n",
        "    - In other words, the bias measures how much the estimate deviates from the true value. "
      ],
      "metadata": {
        "id": "B2zGMk55uIUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)**  \n",
        "Derive the bias-variance trade-off equation for an arbitrary regression model $\\hat{f}(x;\\mathcal{D})$. Treat the dataset as a collection of random variables, so that $\\hat{f}$ is itself a random variable. Assume that the true relationship between the input and output variables is given by $y = f(x) + \\epsilon$, with $f$ a deterministic function and $\\epsilon$ a random variable with $\\mathbb{E}[\\epsilon] = 0$ (zero mean) and $\\text{Var}[\\epsilon] = \\sigma^2$. To be concrete, the starting expression is\n",
        "\n",
        "$$ \\text{MSE}(x) = \\mathbb{E}_{\\epsilon, \\mathcal{D}} \\left[ \\left( y(x) - \\hat{f}(x; \\mathcal{D}) \\right)^2 \\right] \\,. $$"
      ],
      "metadata": {
        "id": "ZYz1Wkwpfisg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your answer here."
      ],
      "metadata": {
        "id": "X3VaozRPhrPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Problem 3: kNN for Handwritten Digits Classification\n",
        "\n",
        "In this problem we will consider classification of handwritten digits. The data (i.e., the $\\boldsymbol{x}_i$) in this case correspond to images, meaning this problem is an example of a computer vision problem. There are specialized methods for such problems that we will cover in a few weeks, but for now we will use the methods we have covered up to this point. "
      ],
      "metadata": {
        "id": "D3Czwyf6hC-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## load the dataset\n",
        "import sklearn, sklearn.datasets\n",
        "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "P-bkkytwdPTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 1797 images in the dataset, and each image corresponds to a 64-dimensional vector. The images were originally digital images with resolution 8x8, but they were flattened, meaning the 8x8 matrices of raw pixel values were converted into 64-dimensional vectors. Each entry in the X data represents a pixel value that can take on an integer value."
      ],
      "metadata": {
        "id": "LXddSn6gi6o8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "5L9gssg0iL_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The y-values correspond to the label (in this case, the digit 0-9). "
      ],
      "metadata": {
        "id": "roXFeVBnhyj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "msibIXEiiJQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize these to gain a better sense for the data"
      ],
      "metadata": {
        "id": "rsh23k76KtV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(7,7))\n",
        "for i in range(16):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(X[i].reshape((8,8)), cmap='binary')\n",
        "    plt.title('digit: %i' %y[i], fontsize=12)\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fIrzWAE0KsqT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)**  \n",
        "\n",
        "What is the minimum and maximum value for $X$? Assuming that each entry can take on an integer value within that range, how many distinct $X$ values (images) are there? Express this number as a power of 10 (i.e., $N = 10^d$ for some $d$)."
      ],
      "metadata": {
        "id": "q9HdkJ-uiIox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "Dd2ps3oakI74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "Having learned about the proper way to do model selection and evaluation, write code that splits the data $(X,y)$ into 3 disjoint sets, \n",
        "\n",
        "$$ (X_{\\text{train}}, y_{\\text{train}}) \\,, \\quad (X_{\\text{val}}, y_{\\text{val}}) \\,, \\quad (X_{\\text{test}}, y_{\\text{test}}) \\,.$$\n",
        "\n",
        "Let's use a splitting ratio of 60/20/20. "
      ],
      "metadata": {
        "id": "uOeLyZYNm_Yp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your answer here"
      ],
      "metadata": {
        "id": "Vy5qDsJXdPgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**  \n",
        "Next, let's train a kNN classifier on this. Using the [docs page for the scikit-learn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), fit a model with $k=5$ to the train data and evaluate the accuracy on all 3 datasets."
      ],
      "metadata": {
        "id": "uF4Fk_4RoTMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors"
      ],
      "metadata": {
        "id": "APjXjmFdpooK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "PIxemce8dPiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part D)**  \n",
        "Let's use the validation set to pick the best $k$ value. For $k=1, ..., 100$, fit a kNN model to the train data, and make a plot of the validation set loss as a function of $k$. What is the optimal value of $k$? \n",
        "\n",
        "For the loss function we will use the cross-entropy loss, as implemented in scikit-learn: [link](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html). Below is some helper code. "
      ],
      "metadata": {
        "id": "xv40EqQcp_ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## helper code\n",
        "## the loss function accepts as inputs the true labels and \n",
        "## the predicted probabilities\n",
        "from sklearn.metrics import log_loss\n",
        "log_loss(y_val, knn.predict_proba(X_val))"
      ],
      "metadata": {
        "id": "gejUhbVdsmf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "rNeBRZWZdPlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part E)**  \n",
        "Using this optimal value of $k$, evaluate the test set accuracy."
      ],
      "metadata": {
        "id": "_wEZ2vz6t6nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## your code here"
      ],
      "metadata": {
        "id": "v51mzPZ2dPqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Problem 4: Gradient Descent\n",
        "\n",
        "In this problem we will revisit the example covered in class (week 2) of gradient descent applied to a quadratic loss function:\n",
        "\n",
        "$$ L(w) = \\frac{1}{2} w^T A w - b^T w \\,. $$\n",
        "\n",
        "If we diagonalize the matrix $A$ and transform to an eigenbasis $x = O w$, with\n",
        "\n",
        "$$ A = O^{-1} \\Lambda O \\,, \\qquad b = O^{-1} b' \\,, \\qquad \\Lambda = (\\lambda_1, ..., \\lambda_d) \\,, $$\n",
        "\n",
        "then we found that the update could be solved for all times\n",
        "    \n",
        "$$ x_a^{t+1} = \\left( 1 - \\alpha \\lambda_a \\right)^{t+1} x_a^0 + \\alpha b_a' \\sum_{k=0}^t \\left( 1 - \\alpha \\lambda_a \\right) \\,. $$\n",
        "\n",
        "Let's assume that the matrix $A$ is positive definite, which means that all the eigenvalues are positive: $\\lambda_a > 0$ for $a = 1, ..., d$. Then, gradient descent will converge if the first term is smaller than 1 in absolute value, which is equivalent to $0 < \\alpha \\lambda_a < 2$. "
      ],
      "metadata": {
        "id": "xqM7qyKZzNBt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part A)**  \n",
        "Prove this, namely that $|1 - \\alpha \\lambda_a| < 1$ implies that $0 < \\alpha \\lambda_a < 2$."
      ],
      "metadata": {
        "id": "2p6exC0lTSMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "nSPrcfXNV74M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inequality we just proved, $0 < \\alpha \\lambda_a < 2$, must hold for all of the indices $a=1,...,d$ in order for gradient descent to converge in all eigendirections. The rate of converence for a single dimension is:\n",
        "\n",
        "$$ \\text{rate}_a(\\alpha) = |1 - \\alpha \\lambda_a| \\,. $$\n",
        "\n",
        "(Note that rate is a potentially confusing term for this, as if this quantity is close to 1, then convergence is very slow, whereas if this quantity is close to 0, then convergence is very rapid. So, in this case a smaller rate corresponds to faster convergence.) \n",
        "\n",
        "The overall rate will be the largest of these, \n",
        "\n",
        "$$ \\text{rate}(\\alpha) = \\max_a |1 - \\alpha \\lambda_a| \\,. $$\n",
        "\n",
        "This expression can be can be simplified to:\n",
        "\n",
        "$$ \\text{rate}(\\alpha) = \\max\\left( |1-\\alpha \\lambda_1|, |1-\\alpha \\lambda_d| \\right) \\,. $$\n",
        "\n",
        "To justify this, recall that $0 < \\alpha \\lambda_a < 2$. The rate vanishes if $\\alpha \\lambda_a = 1$, and it approaches the maximal value of 1 if either $\\alpha \\lambda_a$ is close to zero, or close to 2. Therefore, we can restrict our attention to just the smallest and largest of the eigenvalues. "
      ],
      "metadata": {
        "id": "EWoyNn5_TTEl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part B)**  \n",
        "Given the above expression, what is the optimal learning rate $\\alpha_* = \\text{argmin}_{\\alpha} \\, \\text{rate}(\\alpha)$?"
      ],
      "metadata": {
        "id": "p8OGV9i9dhuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "Emn5GuBzeEZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part C)**  \n",
        "Finally, what is the optimal rate, $\\text{rate}(\\alpha_*)$?"
      ],
      "metadata": {
        "id": "YSDrXfwVfoHw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "your answer here"
      ],
      "metadata": {
        "id": "imy5IjoAfxtg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lus-hl21fyi8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}